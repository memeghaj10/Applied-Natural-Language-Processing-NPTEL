{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 252 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /home/oem/anaconda3/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/oem/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.2.1.tar.gz (122 kB)\n",
      "\u001b[K     |████████████████████████████████| 122 kB 7.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /home/oem/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: requests in /home/oem/anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.15.11.tar.gz (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 1.3 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /home/oem/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/oem/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/oem/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/oem/anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Collecting botocore<1.19.0,>=1.18.11\n",
      "  Downloading botocore-1.18.11-py2.py3-none-any.whl (6.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7 MB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/oem/anaconda3/lib/python3.7/site-packages (from botocore<1.19.0,>=1.18.11->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Building wheels for collected packages: smart-open, boto3\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.2.1-py3-none-any.whl size=114113 sha256=31914cfea4f6c86bb93e3c51ad85b25453caa5e8808d66aa9bd1ed82e8fcd4fa\n",
      "  Stored in directory: /home/oem/.cache/pip/wheels/27/29/59/1e092b4bf54a2a70eb600ec0b5b82a5dee3149019148b15fb0\n",
      "  Building wheel for boto3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for boto3: filename=boto3-1.15.11-py2.py3-none-any.whl size=127859 sha256=d6183d599504653c72f0988689b995ddedebe09577bcd6ba8a0e3aecd3a1a8b7\n",
      "  Stored in directory: /home/oem/.cache/pip/wheels/1a/e9/43/27327b7ca7fcf3cd55e1cce78c70a9eae2bbab67001b73fcfb\n",
      "Successfully built smart-open boto3\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.15.11 botocore-1.18.11 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import os.path\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path,file_name):\n",
    "    documents_list=[]\n",
    "    titles=[]\n",
    "    with open(os.path.join(path,file_name),\"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            text = line.strip()\n",
    "            documents_list.append(text)\n",
    "            \n",
    "    print(\"Total Number of Documents: \",len(documents_list))\n",
    "    titles.append( text[0: min(len(text),100)])\n",
    "    return documents_list,titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocssing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    p_stemmer = PorterStemmer()\n",
    "    texts = []\n",
    "    \n",
    "    for i in doc_set:\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        stopped_tokens = [i for i in tokens if i not in en_stop]\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    \n",
    "    return dictionary,doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an LSA model using Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    dictionary,doc_term_matrix = prepare_corpus(doc_clean)\n",
    "    lsamodel = LsiModel(doc_term_matrix,num_topics=number_of_topics,id2word=dictionary)\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics,num_words=words))\n",
    "    return lsamodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary,doc_term_matrix,doc_clean,stop,start=2,step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, stop, step):\n",
    "        # generate LSA model\n",
    "        model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Coherence Score Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(doc_clean,start,stop,step):\n",
    "    dictionary,doc_term_matrix = prepare_corpus(doc_clean)\n",
    "    model_list,coherence_values = compute_coherence_values(dictionary,doc_term_matrix,doc_clean,stop,start,step)\n",
    "    \n",
    "    x = range(start,stop,step)\n",
    "    plt.plot(x,coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence_Score\")\n",
    "    plt.legend((\"coherence_values\"),loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Documents:  2\n",
      "[(0, '-0.313*\"mother\" + -0.250*\"mathura\" + -0.188*\"meat\" + -0.188*\"told\" + -0.188*\"went\" + -0.188*\"eat\" + -0.188*\"go\" + -0.188*\"place\" + -0.188*\"hotel\" + -0.188*\"cri\"'), (1, '0.408*\"rashmi\" + 0.408*\"kant\" + 0.408*\"portug\" + 0.408*\"receiv\" + 0.408*\"kanji\" + 0.408*\"letter\" + -0.000*\"deiti\" + 0.000*\"forbidden\" + 0.000*\"file\" + -0.000*\"encroach\"')]\n"
     ]
    }
   ],
   "source": [
    "number_of_topics=7\n",
    "words=10\n",
    "document_list,titles=load_data(\"\",\"pep.txt\")\n",
    "clean_text=preprocess_data(document_list)\n",
    "model=create_gensim_lsa_model(clean_text,number_of_topics,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
