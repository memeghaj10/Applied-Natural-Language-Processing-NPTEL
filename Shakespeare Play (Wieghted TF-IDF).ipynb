{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package shakespeare to /home/oem/nltk_data...\n",
      "[nltk_data]   Package shakespeare is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/oem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('shakespeare')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import shakespeare\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import math\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating corpus dictionary\n",
    "corpus_dict={}\n",
    "for file_id in shakespeare.fileids():\n",
    "    corpus_dict[file_id]={} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocsesing Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "ps=PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(terms_list):\n",
    "    return [w.lower() for w in terms_list]\n",
    "\n",
    "def removeAlphaNumeric(terms_list):\n",
    "    return [w for w in terms_list if w.isalpha()]\n",
    "\n",
    "def removeStopWords (terms_list):\n",
    "    return [w for w in terms_list if w not in stop_words]\n",
    "\n",
    "def doStemming (terms_list):\n",
    "    return [ps.stem(w) for w in terms_list]\n",
    "\n",
    "def doLemmatization (terms_list):\n",
    "    return [lemmatizer.lemmatize(w) for w in terms_list]\n",
    "\n",
    "def preProcessWords(terms_list):\n",
    "    \n",
    "    terms_list = normalize_text(terms_list)\n",
    "    terms_list = removeAlphaNumeric(terms_list)\n",
    "    terms_list  = removeStopWords(terms_list)\n",
    "    terms_list = doStemming(terms_list)\n",
    "    terms_list = doLemmatization(terms_list)\n",
    "    \n",
    "    return terms_list\n",
    "\n",
    "for file_id in shakespeare.fileids():\n",
    "    doc  = corpus_dict[file_id]\n",
    "    play_terms = shakespeare.words(file_id)\n",
    "    doc['words'] = preProcessWords(play_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary set for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabulary (doc_terms_list):\n",
    "    \n",
    "    return set(doc_terms_list)\n",
    "\n",
    "for file_id in shakespeare.fileids():\n",
    "    doc=corpus_dict[file_id]\n",
    "    doc['vocab'] = getVocabulary(doc['words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Word Frequency List for Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCounts(doc_terms_list):\n",
    "    counts={}\n",
    "    for w in doc_terms_list:\n",
    "        count = counts.get(w,0)\n",
    "        counts[w]=count+1\n",
    "    return counts\n",
    "\n",
    "for file_id in shakespeare.fileids():\n",
    "    doc = corpus_dict[file_id]\n",
    "    doc['counts'] = getWordCounts(doc['words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Term Frequency List for Each Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTermFrequency(doc_vocab_list, doc_count_dict, T):\n",
    "    '''\n",
    "    Input:\n",
    "        doc_vocab_list: Document Vocabulary as Iterable\n",
    "        doc_count_dict: Document Word Frequency as Dictionary\n",
    "        T: Total Number of Words in the Document\n",
    "    Output:\n",
    "        Returns a Dictionary, having\n",
    "            Each Term as a Key\n",
    "            Term Frequency of each Term as its value\n",
    "            TF = C/T\n",
    "                C: Count of the Term\n",
    "                T: Total Words In Document\n",
    "    '''\n",
    "    tf = {}\n",
    "    for w in doc_vocab_list:\n",
    "        tf[w] = doc_count_dict[w] / T\n",
    "    return tf;\n",
    "\n",
    "for file_id in shakespeare.fileids():\n",
    "    doc = corpus_dict[file_id]\n",
    "    doc['tf'] = getTermFrequency(doc['vocab'],doc['counts'], len(doc['words']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Corpus Vocabulary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalVocabList(corpus_dict):\n",
    "    total_corpus_vocab = set()\n",
    "    for k,v in corpus_dict.items():\n",
    "        total_corpus_vocab = total_corpus_vocab.union(corpus_dict[k]['vocab'])\n",
    "    return total_corpus_vocab\n",
    "\n",
    "vocab_list = []\n",
    "\n",
    "for file_id in shakespeare.fileids():\n",
    "    doc=corpus_dict[file_id]\n",
    "    vocab_list.append(doc['vocab'])\n",
    "    \n",
    "from itertools import chain\n",
    "corpus_total_vocab = set(chain.from_iterable(vocab_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Document Frequency List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocumentFrequency(corpus_dict, total_corpus_vocab):\n",
    "    '''\n",
    "    from the given docs dicts,\n",
    "    construct Document Frequency list\n",
    "    and returns it\n",
    "    '''\n",
    "    df = {}\n",
    "    for w in total_corpus_vocab:\n",
    "        df_count = 0\n",
    "        for file_id,doc in corpus_dict.items():\n",
    "            if w in corpus_dict[file_id]['vocab']:\n",
    "                df_count = df_count + 1\n",
    "        df[w] = df_count\n",
    "    return df\n",
    "            \n",
    "corpus_df = getDocumentFrequency(corpus_dict, corpus_total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def getInverseDocumentFrequency(corpus_df, total_corpus_vocab):\n",
    "    '''\n",
    "    using provided document frequency\n",
    "    constructs IDF for each vocab in total_vocab\n",
    "    and returns it as dict\n",
    "    '''\n",
    "    idf = {}\n",
    "    for w in total_corpus_vocab:\n",
    "        idf[w] = corpus_df[w] / len(shakespeare.fileids())\n",
    "    return idf\n",
    "\n",
    "corpus_idf = getInverseDocumentFrequency(corpus_df, corpus_total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Wieghted TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeightedTFIDFOfATerm(tc, idf):\n",
    "   \n",
    "    wv = 0\n",
    "    if(tc > 0):\n",
    "        wv = 1 + np.log10(tc)\n",
    "    #print('tc: ', tc, 'idf: ', idf, 'wf-idf: ', wv * idf)\n",
    "    return wv * idf\n",
    "\n",
    "def getWeightedTFIDFOfADocument(doc_vocab, doc_count, idf):\n",
    "   \n",
    "    wv_tf_idf = {}\n",
    "    for w in doc_vocab:\n",
    "        wv_tf_idf[w] = getWeightedTFIDFOfATerm(doc_count[w], idf[w])\n",
    "    return wv_tf_idf\n",
    "\n",
    "for file_id,doc in corpus_dict.items():\n",
    "    vocab = corpus_dict[file_id]['vocab']\n",
    "    counts = corpus_dict[file_id]['counts']\n",
    "    doc = corpus_dict[file_id]\n",
    "    doc['wv_tf_idf'] = getWeightedTFIDFOfADocument(vocab, counts, corpus_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Weighted TF-IDF DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_headers = shakespeare.fileids()\n",
    "tfidf_matrix = pd.DataFrame(columns=col_headers)\n",
    "\n",
    "for w in corpus_total_vocab:\n",
    "    wv = []\n",
    "    for file_id,doc in corpus_dict.items():\n",
    "        wv.append(doc['wv_tf_idf'].get(w, 0))\n",
    "    tfidf_matrix.loc[w] = wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_and_c.xml</th>\n",
       "      <th>dream.xml</th>\n",
       "      <th>hamlet.xml</th>\n",
       "      <th>j_caesar.xml</th>\n",
       "      <th>macbeth.xml</th>\n",
       "      <th>merchant.xml</th>\n",
       "      <th>othello.xml</th>\n",
       "      <th>r_and_j.xml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abat</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.650515</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abbey</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abhor</th>\n",
       "      <td>0.62500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.62500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923201</td>\n",
       "      <td>0.813144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abi</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.162629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>younker</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>1.69897</td>\n",
       "      <td>1.845098</td>\n",
       "      <td>2.204120</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.954243</td>\n",
       "      <td>1.698970</td>\n",
       "      <td>1.954243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeal</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zound</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.369280</td>\n",
       "      <td>0.325257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7554 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         a_and_c.xml  dream.xml  hamlet.xml  j_caesar.xml  macbeth.xml  \\\n",
       "abandon      0.00000   0.000000    0.000000       0.00000      0.00000   \n",
       "abat         0.00000   0.500000    0.650515       0.00000      0.00000   \n",
       "abbey        0.00000   0.000000    0.000000       0.00000      0.00000   \n",
       "abhor        0.62500   0.000000    0.625000       0.00000      0.62500   \n",
       "abi          0.00000   0.162629    0.000000       0.00000      0.00000   \n",
       "...              ...        ...         ...           ...          ...   \n",
       "younker      0.00000   0.000000    0.000000       0.00000      0.00000   \n",
       "youth        1.69897   1.845098    2.204120       1.30103      1.30103   \n",
       "zeal         0.00000   0.000000    0.000000       0.00000      0.00000   \n",
       "zone         0.00000   0.000000    0.125000       0.00000      0.00000   \n",
       "zound        0.00000   0.000000    0.000000       0.00000      0.00000   \n",
       "\n",
       "         merchant.xml  othello.xml  r_and_j.xml  \n",
       "abandon      0.000000     0.125000     0.000000  \n",
       "abat         0.500000     0.000000     0.500000  \n",
       "abbey        0.000000     0.000000     0.125000  \n",
       "abhor        0.000000     0.923201     0.813144  \n",
       "abi          0.000000     0.000000     0.000000  \n",
       "...               ...          ...          ...  \n",
       "younker      0.125000     0.000000     0.000000  \n",
       "youth        1.954243     1.698970     1.954243  \n",
       "zeal         0.125000     0.000000     0.000000  \n",
       "zone         0.000000     0.000000     0.000000  \n",
       "zound        0.000000     0.369280     0.325257  \n",
       "\n",
       "[7554 rows x 8 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Query Vocabulary\n",
    "    Randomly selecting query words from Corpus Vocabulary\n",
    "    Out-of-Vocabulary (OOV) is not considered in this test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_query_words:  20\n"
     ]
    }
   ],
   "source": [
    "max_query_words = 20\n",
    "print('max_query_words: ', max_query_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_words:  ['earthquak', 'confessor', 'bewhor', 'gobbo', 'composur', 'tribe', 'patienc', 'seedsman', 'gash', 'pomegran', 'distress', 'avenu', 'afor', 'alexa', 'grimli', 'treacheri', 'laudabl', 'haggard', 'demon', 'reckon']\n"
     ]
    }
   ],
   "source": [
    "query_words = random.choices(list(corpus_total_vocab), k=max_query_words)\n",
    "print('query_words: ', query_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc = {}\n",
    "query_doc['words'] = preProcessWords(query_words)\n",
    "query_doc['vocab'] = getVocabulary(query_doc['words'])\n",
    "query_doc['counts'] = getWordCounts(query_doc['words'])\n",
    "query_doc['tf'] = getTermFrequency(query_doc['vocab'], query_doc['counts'], len(query_doc['words']))\n",
    "query_doc['wv_tf_idf'] = getWeightedTFIDFOfADocument(query_doc['vocab'], query_doc['counts'], corpus_idf)\n",
    "\n",
    "# for terms that don't exist in query words, assign 0 as Weighted TF-IDF for those terms\n",
    "for w in corpus_total_vocab:\n",
    "    count = query_doc['wv_tf_idf'].get(w, 0)\n",
    "    query_doc['wv_tf_idf'][w] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a, b):\n",
    "    return 1 - cosine_similarity(a,b)\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7554,)\n",
      "(7554,)\n",
      "(7554,)\n",
      "(7554,)\n",
      "(7554,)\n",
      "(7554,)\n",
      "(7554,)\n",
      "(7554,)\n"
     ]
    }
   ],
   "source": [
    "query_tf_df_matrix = pd.DataFrame(columns=['Wv-TF-IDF'])\n",
    "\n",
    "for file_id,doc in corpus_dict.items():\n",
    "    cos_sim= []\n",
    "    doc_val = np.array(tfidf_matrix[file_id]).T\n",
    "    pprint(doc_val.shape)\n",
    "    query_val = np.array(list(query_doc['wv_tf_idf'].values()))\n",
    "    query_tf_df_matrix.loc[file_id] = cosine_similarity(doc_val, query_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tf_df_matrix.sort_values(by='Wv-TF-IDF',ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wv-TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>r_and_j.xml</th>\n",
       "      <td>0.020376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a_and_c.xml</th>\n",
       "      <td>0.017167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>othello.xml</th>\n",
       "      <td>0.016528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hamlet.xml</th>\n",
       "      <td>0.015866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macbeth.xml</th>\n",
       "      <td>0.014134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merchant.xml</th>\n",
       "      <td>0.014108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dream.xml</th>\n",
       "      <td>0.008535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j_caesar.xml</th>\n",
       "      <td>0.007037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Wv-TF-IDF\n",
       "r_and_j.xml    0.020376\n",
       "a_and_c.xml    0.017167\n",
       "othello.xml    0.016528\n",
       "hamlet.xml     0.015866\n",
       "macbeth.xml    0.014134\n",
       "merchant.xml   0.014108\n",
       "dream.xml      0.008535\n",
       "j_caesar.xml   0.007037"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_tf_df_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
